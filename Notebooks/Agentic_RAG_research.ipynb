{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -Uqq langgraph langchain-google-genai llama-index-embeddings-huggingface llama-index streamlit requests beautifulsoup4 arxiv scholarly langchain-core langchain-community chromadb llama-index-vector-stores-chroma google-ai-generativelanguage sentence-transformers free-proxy feedparser retry","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:22:24.582610Z","iopub.execute_input":"2025-04-24T09:22:24.582939Z","iopub.status.idle":"2025-04-24T09:24:58.409941Z","shell.execute_reply.started":"2025-04-24T09:22:24.582915Z","shell.execute_reply":"2025-04-24T09:24:58.408281Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.3/187.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!rm -f papers_cache.json # Clearing cache to avoid stale data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:24:58.412428Z","iopub.execute_input":"2025-04-24T09:24:58.412819Z","iopub.status.idle":"2025-04-24T09:24:58.543539Z","shell.execute_reply.started":"2025-04-24T09:24:58.412787Z","shell.execute_reply":"2025-04-24T09:24:58.542177Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!rm -f papers_cache.json # Clearing cache to avoid stale data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:24:58.544796Z","iopub.execute_input":"2025-04-24T09:24:58.545046Z","iopub.status.idle":"2025-04-24T09:24:58.667561Z","shell.execute_reply.started":"2025-04-24T09:24:58.545022Z","shell.execute_reply":"2025-04-24T09:24:58.666415Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Step-by-step Implementation","metadata":{}},{"cell_type":"code","source":"##@ All imports here:\nfrom scholarly import scholarly\nimport arxiv\nimport time\nimport os\nimport json\nimport requests\nfrom llama_index.core import VectorStoreIndex, Document, SimpleDirectoryReader\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import Settings\nimport chromadb\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom typing import TypedDict, Sequence\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom IPython.display import Image, display\nfrom retry import retry\nimport logging","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:24:58.669950Z","iopub.execute_input":"2025-04-24T09:24:58.670251Z","iopub.status.idle":"2025-04-24T09:25:39.473201Z","shell.execute_reply.started":"2025-04-24T09:24:58.670225Z","shell.execute_reply":"2025-04-24T09:25:39.472271Z"}},"outputs":[{"name":"stderr","text":"2025-04-24 09:25:20.130509: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745486720.349677      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745486720.411031      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#@ Setting up logging\nlogging.basicConfig(filename=\"paper_fetcher.log\", level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n#! Embedding model ...\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:25:39.474196Z","iopub.execute_input":"2025-04-24T09:25:39.475347Z","iopub.status.idle":"2025-04-24T09:25:48.885173Z","shell.execute_reply.started":"2025-04-24T09:25:39.475319Z","shell.execute_reply":"2025-04-24T09:25:48.884173Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc0e067ceae4707ab25f794a9f31fbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0720e1f4fc5549dea7b00b043b574b91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63b279ed65548a0b268bcb44f1ade6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"019e31f7d6c6432cae24990e3c9be9fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e535adf63754955835567d188ca5ceb"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0da40d862b646048ea33aaf7aca3b84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bff25d407d6e4bd5ae5c0536796a170e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb62d493db154ef7ba96cb06aa75eb4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15332818fb9f4eeaa3ca59337503deb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f1bc1a6cfa9422aa5a8f2f0c78c6249"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddb3f2bac24c41dd8c77f2b5c5e3083d"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"##@ fetching papers from multiple sources\n#! Arxiv First well for others Im going to use google scholar to fetch other pubs papers as well: like IEE and research gate\n@retry(tries=1, delay=1, backoff=2)  # Single retry\ndef fetch_arxiv_papers(query=\"AI agents\", max_results=10):\n    try:\n        start_time = time.time()\n        logging.info(f\"Starting arXiv fetch for query: {query}\")\n        client = arxiv.Client()\n        search = arxiv.Search(\n            query=query,\n            max_results=max_results,\n            sort_by=arxiv.SortCriterion.SubmittedDate\n        )\n        papers = []\n        for result in client.results(search):\n            papers.append({\n                \"title\": result.title,\n                \"abstract\": result.summary,\n                \"url\": result.pdf_url,\n                \"published\": result.published.isoformat(),  # Convert datetime to string\n                \"source\": \"arXiv\"\n            })\n        logging.info(f\"Fetched {len(papers)} papers from arXiv in {time.time() - start_time:.2f} seconds\")\n        return papers\n    except Exception as e:\n        logging.error(f\"Error fetching arXiv papers: {e}\")\n        return []\n\n@retry(tries=1, delay=1, backoff=2)  # Single retry\ndef fetch_ieee_papers(query=\"AI agents\", max_results=10):\n    try:\n        start_time = time.time()\n        logging.info(f\"Starting IEEE fetch for query: {query}\")\n        # Use requests with proxy\n        proxies = {\n            'http': 'http://123.45.67.89:8080',\n            'https': 'https://123.45.67.89:8080'\n        }\n        session = requests.Session()\n        session.proxies.update(proxies)\n        search_query = scholarly.search_pubs(f\"{query} site:*.ieee.org\", timeout=10) \n        papers = []\n        for i, pub in enumerate(search_query):\n            if i >= max_results:\n                break\n            time.sleep(10)  # 10s sleep\n            # Ensuring the result is from IEEE\n            if \"ieee.org\" in pub.get(\"pub_url\", \"\").lower():\n                papers.append({\n                    \"title\": pub[\"bib\"][\"title\"],\n                    \"abstract\": pub.get(\"abstract\", \"\"),\n                    \"url\": pub.get(\"pub_url\", \"\"),\n                    \"source\": \"IEEE\"\n                })\n        logging.info(f\"Fetched {len(papers)} papers from IEEE in {time.time() - start_time:.2f} seconds\")\n        return papers\n    except Exception as e:\n        logging.error(f\"Error fetching IEEE papers: {e}\")\n        return []\n\n@retry(tries=1, delay=1, backoff=2)  # Single retry\ndef fetch_researchgate_papers(query=\"AI agents\", max_results=10):\n    try:\n        start_time = time.time()\n        logging.info(f\"Starting ResearchGate fetch for query: {query}\")\n        # Use requests with proxy\n        proxies = {\n            'http': 'http://123.45.67.89:8080',\n            'https': 'https://123.45.67.89:8080'\n        }\n        session = requests.Session()\n        session.proxies.update(proxies)\n        search_query = scholarly.search_pubs(f\"{query} site:*.researchgate.net\", timeout=10)  # 10s timeout\n        papers = []\n        for i, pub in enumerate(search_query):\n            if i >= max_results:\n                break\n            time.sleep(10)  # 10s sleep\n            # Ensuring the result is from ResearchGate\n            if \"researchgate.net\" in pub.get(\"pub_url\", \"\").lower():\n                papers.append({\n                    \"title\": pub[\"bib\"][\"title\"],\n                    \"abstract\": pub.get(\"abstract\", \"\"),\n                    \"url\": pub.get(\"pub_url\", \"\"),\n                    \"source\": \"ResearchGate\"\n                })\n        logging.info(f\"Fetched {len(papers)} papers from ResearchGate in {time.time() - start_time:.2f} seconds\")\n        return papers\n    except Exception as e:\n        logging.error(f\"Error fetching ResearchGate papers: {e}\")\n        return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:25:48.886347Z","iopub.execute_input":"2025-04-24T09:25:48.886797Z","iopub.status.idle":"2025-04-24T09:25:48.902873Z","shell.execute_reply.started":"2025-04-24T09:25:48.886759Z","shell.execute_reply":"2025-04-24T09:25:48.901732Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"##@ Combining all those to make a single fetching function\ndef fetch_all_papers(query=\"AI agents\", max_results=10):\n    try:\n        start_time = time.time()\n        logging.info(f\"Starting fetch_all_papers for query: {query}\")\n\n        # Check cache\n        cache_file = \"papers_cache.json\"\n        if os.path.exists(cache_file):\n            with open(cache_file, \"r\") as f:\n                cached_papers = json.load(f)\n                if cached_papers.get(\"query\") == query and cached_papers.get(\"max_results\") == max_results:\n                    logging.info(f\"Returning cached papers in {time.time() - start_time:.2f} seconds\")\n                    return cached_papers[\"papers\"]\n\n        # Fetch papers from all sources\n        arxiv_papers = fetch_arxiv_papers(query, max_results)\n        logging.info(f\"Completed arXiv fetch in {time.time() - start_time:.2f} seconds\")\n        ieee_papers = fetch_ieee_papers(query, max_results)\n        logging.info(f\"Completed IEEE fetch in {time.time() - start_time:.2f} seconds\")\n        researchgate_papers = fetch_researchgate_papers(query, max_results)\n        logging.info(f\"Completed ResearchGate fetch in {time.time() - start_time:.2f} seconds\")\n\n        all_papers = arxiv_papers + ieee_papers + researchgate_papers\n\n        # Deduplicating by title (incase the papers are published in multiple sources)\n        seen_titles = set()\n        unique_papers = []\n        for paper in all_papers:\n            if paper[\"title\"].lower() not in seen_titles:\n                seen_titles.add(paper[\"title\"].lower())\n                unique_papers.append(paper)\n\n        # Cache results\n        with open(cache_file, \"w\") as f:\n            json.dump({\"query\": query, \"max_results\": max_results, \"papers\": unique_papers}, f)\n        logging.info(f\"Cached {len(unique_papers)} papers in {time.time() - start_time:.2f} seconds\")\n\n        return unique_papers\n    except Exception as e:\n        logging.error(f\"Error combining papers: {e}\")\n        # Fallback to arXiv only\n        arxiv_papers = fetch_arxiv_papers(query, max_results)\n        all_papers = arxiv_papers\n        seen_titles = set()\n        unique_papers = []\n        for paper in all_papers:\n            if paper[\"title\"].lower() not in seen_titles:\n                seen_titles.add(paper[\"title\"].lower())\n                unique_papers.append(paper)\n        with open(cache_file, \"w\") as f:\n            json.dump({\"query\": query, \"max_results\": max_results, \"papers\": unique_papers}, f)\n        logging.info(f\"Fallback: Cached {len(unique_papers)} arXiv papers in {time.time() - start_time:.2f} seconds\")\n        return unique_papers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:25:48.903996Z","iopub.execute_input":"2025-04-24T09:25:48.904291Z","iopub.status.idle":"2025-04-24T09:25:48.933742Z","shell.execute_reply.started":"2025-04-24T09:25:48.904260Z","shell.execute_reply":"2025-04-24T09:25:48.932868Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"@retry(tries=1, delay=1, backoff=2)  # Single retry\ndef download_pdf(url, filename):\n    try:\n        start_time = time.time()\n        logging.info(f\"Starting PDF download: {url}\")\n        response = requests.get(url, stream=True, timeout=5)  # 5-second timeout\n        response.raise_for_status()\n        with open(filename, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n        logging.info(f\"Downloaded PDF {filename} in {time.time() - start_time:.2f} seconds\")\n    except Exception as e:\n        logging.error(f\"Failed to download PDF {url}: {e}\")\n        raise\n\ndef index_papers(papers, collection_name=\"ai_papers\"):\n    ##! Initializing chromaindex\n    try:\n        start_time = time.time()\n        logging.info(f\"Starting indexing for collection: {collection_name}\")\n        chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n        chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n\n        ##! Initializing the chromavectorstore for llamaindex\n        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n\n        documents = []\n        os.makedirs(\"./papers\", exist_ok=True)\n\n        for i, paper in enumerate(papers):\n            # Validate paper data\n            if not paper.get(\"title\") or not paper.get(\"abstract\"):\n                logging.warning(f\"Skipping paper {i} due to missing title or abstract\")\n                continue\n\n            doc = Document(\n                text=paper[\"abstract\"],\n                metadata={\n                    \"title\": paper[\"title\"],\n                    \"url\": paper[\"url\"],\n                    \"source\": paper[\"source\"]\n                }\n            )\n            documents.append(doc)\n\n            ## Now optionally we index the arxiv paper pdfs as well\n            ## And yea found out only arxiv provides the pdf_url :)\n            if paper[\"source\"] == \"arXiv\" and paper[\"url\"]:\n                filename = f\"./papers/paper_{i}.pdf\"\n                try:\n                    download_pdf(paper[\"url\"], filename)\n                    pdf_docs = SimpleDirectoryReader(input_files=[filename]).load_data()\n                    for pdf_doc in pdf_docs:\n                        pdf_doc.metadata.update(doc.metadata)\n                        documents.append(pdf_doc)\n                except Exception as e:\n                    logging.error(f\"Failed to process PDF {filename}: {e}\")\n\n        ## Finally: vectorstoreindex with chromadb\n        from llama_index.core import StorageContext\n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n        index = VectorStoreIndex.from_documents(\n            documents=documents,\n            storage_context=storage_context\n        )\n        logging.info(f\"Indexed {len(documents)} documents in collection {collection_name} in {time.time() - start_time:.2f} seconds\")\n        return index, chroma_collection\n    except Exception as e:\n        logging.error(f\"Error indexing papers: {e}\")\n        return None, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:25:48.934833Z","iopub.execute_input":"2025-04-24T09:25:48.935096Z","iopub.status.idle":"2025-04-24T09:25:48.958317Z","shell.execute_reply.started":"2025-04-24T09:25:48.935068Z","shell.execute_reply":"2025-04-24T09:25:48.957271Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ngemini_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = gemini_key","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:26:03.818670Z","iopub.execute_input":"2025-04-24T09:26:03.819019Z","iopub.status.idle":"2025-04-24T09:26:03.981880Z","shell.execute_reply.started":"2025-04-24T09:26:03.818995Z","shell.execute_reply":"2025-04-24T09:26:03.980892Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"##@ Okay now time for langgraph:\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom typing import TypedDict, Sequence\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom IPython.display import Image, display\n# Define state\nclass AgentState(TypedDict):\n    messages: Sequence[BaseMessage]\n    documents: list\n\n# Initialize LLM\nllm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n\n# Retrieval tool\ndef retrieve(state):\n    query = state[\"messages\"][0].content\n    retriever = index.as_retriever(similarity_top_k=3)\n    docs = retriever.retrieve(query)\n    logging.info(f\"Retrieved {len(docs)} documents for query: {query}\")\n    return {\"documents\": [doc.text for doc in docs], \"messages\": state[\"messages\"]}\n\n# Grade documents\ndef grade_documents(state):\n    docs = state[\"documents\"]\n    query = state[\"messages\"][0].content\n    prompt = PromptTemplate(\n        input_variables=[\"query\", \"docs\"],\n        template=\"Are these documents relevant to the query '{query}'? Answer 'yes' or 'no'.\"\n    )\n    chain = prompt | llm | StrOutputParser()\n    response = chain.invoke({\"query\": query, \"docs\": \"\\n\".join(docs)})\n    logging.info(f\"Document grading result: {response}\")\n    return {\"documents\": docs if response.lower() == \"yes\" else [], \"messages\": state[\"messages\"]}\n\n# Generate response\ndef generate(state):\n    docs = state[\"documents\"]\n    query = state[\"messages\"][0].content\n    prompt = PromptTemplate(\n        input_variables=[\"context\", \"question\"],\n        template=\"Answer the question based on the context:\\nContext: {context}\\nQuestion: {question}\"\n    )\n    chain = prompt | llm | StrOutputParser()\n    response = chain.invoke({\"context\": \"\\n\".join(docs), \"question\": query})\n    logging.info(f\"Generated response for query: {query}\")\n    return {\"messages\": [HumanMessage(content=response)]}\n\n# Web search fallback (optional)\ndef web_search(state):\n    query = state[\"messages\"][0].content\n    logging.warning(\"Web search not implemented (placeholder)\")\n    return {\"documents\": [], \"messages\": state[\"messages\"]}\n\n# Conditional routing\ndef route(state):\n    docs = state[\"documents\"]\n    return \"generate\" if docs else \"web_search\"\n\n# Define graph\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"retrieve\", retrieve)\nworkflow.add_node(\"grade\", grade_documents)\nworkflow.add_node(\"generate\", generate)\nworkflow.add_node(\"web_search\", web_search)\n\nworkflow.add_edge(START, \"retrieve\")\nworkflow.add_edge(\"retrieve\", \"grade\")\nworkflow.add_conditional_edges(\"grade\", route, {\"generate\": \"generate\", \"web_search\": \"web_search\"})\nworkflow.add_edge(\"web_search\", \"generate\")\nworkflow.add_edge(\"generate\", END)\n\ngraph = workflow.compile()\n\n# display(Image(graph.get_graph().draw_mermaid_png()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:26:06.006989Z","iopub.execute_input":"2025-04-24T09:26:06.007811Z","iopub.status.idle":"2025-04-24T09:26:06.121845Z","shell.execute_reply.started":"2025-04-24T09:26:06.007774Z","shell.execute_reply":"2025-04-24T09:26:06.120937Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"query = \"AI agents\"\npapers = fetch_all_papers(query, max_results=5)\nfor paper in papers:\n    print(f\"Title: {paper['title']}\")\n    print(f\"Abstract: {paper['abstract'][:100]}...\")\n    print(f\"URL: {paper['url']}\")\n    print(f\"Source: {paper['source']}\")\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:26:09.089484Z","iopub.execute_input":"2025-04-24T09:26:09.089830Z","iopub.status.idle":"2025-04-24T09:26:10.227335Z","shell.execute_reply.started":"2025-04-24T09:26:09.089807Z","shell.execute_reply":"2025-04-24T09:26:10.226254Z"}},"outputs":[{"name":"stdout","text":"Title: Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light\nAbstract: Many sparse attention mechanisms such as Neighborhood Attention have\ntypically failed to consistentl...\nURL: http://arxiv.org/pdf/2504.16922v1\nSource: arXiv\n---\nTitle: OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents\nAbstract: Optimization plays a vital role in scientific research and practical\napplications, but formulating a...\nURL: http://arxiv.org/pdf/2504.16918v1\nSource: arXiv\n---\nTitle: Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text\nAbstract: In recent years, the detection of AI-generated text has become a critical\narea of research due to co...\nURL: http://arxiv.org/pdf/2504.16913v1\nSource: arXiv\n---\nTitle: Building A Secure Agentic AI Application Leveraging A2A Protocol\nAbstract: As Agentic AI systems evolve from basic workflows to complex multi agent\ncollaboration, robust proto...\nURL: http://arxiv.org/pdf/2504.16902v1\nSource: arXiv\n---\nTitle: AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset\nAbstract: This paper presents our winning submission to the AI Mathematical Olympiad -\nProgress Prize 2 (AIMO-...\nURL: http://arxiv.org/pdf/2504.16891v1\nSource: arXiv\n---\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"## Testing the indexing with Chromadb\n# Test indexing\nindex, chroma_collection = index_papers(papers, collection_name=\"ai_papers_test\")\nif index:\n    print(\"Indexing complete. Collection:\", chroma_collection.name)\n\n    # Test retrieval\n    retriever = index.as_retriever(similarity_top_k=3)\n    docs = retriever.retrieve(\"What are AI agents?\")\n    for doc in docs:\n        print(f\"Text: {doc.text[:100]}...\")\n        print(f\"Metadata: {doc.metadata}\")\n        print(\"---\")\nelse:\n    print(\"Indexing failed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T09:26:13.317224Z","iopub.execute_input":"2025-04-24T09:26:13.317592Z","iopub.status.idle":"2025-04-24T09:26:34.243903Z","shell.execute_reply.started":"2025-04-24T09:26:13.317566Z","shell.execute_reply":"2025-04-24T09:26:34.242913Z"}},"outputs":[{"name":"stdout","text":"Indexing complete. Collection: ai_papers_test\nText: Fig. 1. Maestro Architecture - 7 Layers\n• AI-Specific Threats: Focuses on the unique threats arising...\nMetadata: {'page_label': '5', 'file_name': 'paper_3.pdf', 'file_path': 'papers/paper_3.pdf', 'file_type': 'application/pdf', 'file_size': 1761759, 'creation_date': '2025-04-24', 'last_modified_date': '2025-04-24', 'title': 'Building A Secure Agentic AI Application Leveraging A2A Protocol', 'url': 'http://arxiv.org/pdf/2504.16902v1', 'source': 'arXiv'}\n---\nText: of research, we propose to integrate a multi-agent architecture into our system design to\nenable mor...\nMetadata: {'page_label': '6', 'file_name': 'paper_1.pdf', 'file_path': 'papers/paper_1.pdf', 'file_type': 'application/pdf', 'file_size': 1098105, 'creation_date': '2025-04-24', 'last_modified_date': '2025-04-24', 'title': 'OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents', 'url': 'http://arxiv.org/pdf/2504.16918v1', 'source': 'arXiv'}\n---\nText: REFERENCES\n[1] K. Huang, “Agentic AI threat modeling framework:\nMAESTRO,” https://cloudsecurityallia...\nMetadata: {'page_label': '12', 'file_name': 'paper_3.pdf', 'file_path': 'papers/paper_3.pdf', 'file_type': 'application/pdf', 'file_size': 1761759, 'creation_date': '2025-04-24', 'last_modified_date': '2025-04-24', 'title': 'Building A Secure Agentic AI Application Leveraging A2A Protocol', 'url': 'http://arxiv.org/pdf/2504.16902v1', 'source': 'arXiv'}\n---\n","output_type":"stream"}],"execution_count":15}]}